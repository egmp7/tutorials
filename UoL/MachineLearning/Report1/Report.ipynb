{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76ad1f83-1a96-4708-af0c-db211732232e",
   "metadata": {},
   "source": [
    "# CM3015 Mid-term Coursework Report\r\n",
    "\r\n",
    "## Abstract\r\n",
    "This report aims to identify the best machine learning model for the breast cancer dataset obtained from the scikit-learn library. The study employs a comprehensive approach by implementing a k-Nearest Neighbors (KNN) algorithm from scratch, utilizing scikit-learn's KNN, and applying Decision Trees Classification from the same library. The report summarizes the findings and evaluations of these machine learning models.\r\n",
    "\r\n",
    "## 1. Introduction\r\n",
    "Machine learning plays a crucial role in healthcare, and the identification of the best model for breast cancer classification is of paramount importance. This project explores the application of various machine learning algorithms to the breast cancer dataset, aiming to determine the most effective lication, and implementation.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e428ca19-3db3-43ba-815c-75349cb5f698",
   "metadata": {},
   "source": [
    "## 2. Background\n",
    "In this study, two prominent machine learning algorithms, k-Nearest Neighbors (KNN) and Decision Trees Classification (DTC), are explored for their efficacy in breast cancer classification. KNN, a proximity-based algorithm, assigns a data point the majority class of its k-nearest neighbors. Additionally, a scratch implementation of KNN is developed for comparison. On the other hand, DTC constructs a tree-like model, recursively partitioning the feature space to make decisions. Both algorithms offer unique strengths and interpretability, and their performance will be rigorously compared to determine the most effective approach for breast cancer classification. The scratch implementation of KNN introduces an additional layer of analysis, providing insights into algorithmic intricacies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9122037-2d55-4813-a240-290adda2b39f",
   "metadata": {},
   "source": [
    "## 3. Methodology\n",
    "We started by loading the breast cancer dataset from scikit-learn and preprocessing the data as necessary. The methodology includes the implementation of a KNN algorithm from scratch using standard Python code, alongside the application of scikit-learn's KNN and Decision Trees Classification. The exploration involves techniques such as cross-validation to ensure robust evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ac343a-c3f8-4d16-a447-7c585c38da7b",
   "metadata": {},
   "source": [
    "### 3.1 KNN from the scratch\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm is implemented from scratch in Python, incorporating essential functions for distance calculation and neighbor selection. The methodology involves the following steps:\n",
    "\n",
    "1. **Euclidean Distance Calculation:** The euclidean_distance() function computes the Euclidean distance between two data points.\n",
    "2. **Neighbor Selection:** The get_neighbors() function identifies the k-nearest neighbors of a given test data point within the training dataset based on Euclidean distance.\n",
    "3. **Classification Prediction:** The predict_classification() function predicts the class label of the test data point using a majority voting mechanism among its neighbors.\n",
    "\n",
    "The choice of the number of neighbors (k) is crucial, and a thorough evaluation is conducted to determine the optimal value. The methodology ensures a comprehensive understanding of the implemented KNN algorithm and its performance characteristics on the breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5e35f4eb-f657-4481-b196-d6fb8c42dda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "356f572c-b4a1-4360-9c2f-619ed940a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distance between rows\n",
    "def euclidean_distance(row1, row2):\n",
    "    return np.sqrt(np.sum((row1 - row2)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "617e9fd9-c0b4-421d-9814-5e3c29574246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the most similar neighbors\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "    \n",
    "    distances = list()\n",
    "    \n",
    "    for i, train_row in enumerate(train):\n",
    "        \n",
    "        dist = euclidean_distance(test_row, train_row)\n",
    "        distances.append((i, dist))\n",
    "        distances.sort(key=lambda tup: tup[1])\n",
    "\n",
    "    neighbors = [index for index, _ in distances[:num_neighbors]]\n",
    "    \n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b467b9ce-f896-4187-b12e-ac769bd65f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a classification prediction with neighbors\n",
    "def predict_classification(train, test_row, num_neighbors, y_train):\n",
    "    \n",
    "    negativeCounter = 0\n",
    "    positiveCounter = 0\n",
    "    \n",
    "    neighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "    \n",
    "    for n in neighbors:\n",
    "        if (y_train[n] == 0): negativeCounter += 1\n",
    "        if (y_train[n] == 1): positiveCounter += 1\n",
    "\n",
    "    if (negativeCounter > positiveCounter): return 0\n",
    "    if (negativeCounter < positiveCounter): return 1\n",
    "    if (negativeCounter == positiveCounter): print(\"choose anoter value for K\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17386457-3757-4ddd-8b06-86d7940c14dd",
   "metadata": {},
   "source": [
    "### 3.1 Scaling \n",
    "\n",
    "The breast cancer dataset (X) is loaded, consisting of various features. To standardize the feature values, the StandardScaler from scikit-learn is employed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a1ab76f1-5e82-4a62-98a9-e406c3e30d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "# Get features\n",
    "X = data.data\n",
    "# Instance of scaler\n",
    "scaler = StandardScaler()\n",
    "# Scaled features\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f97958-0f20-4636-b11e-d3921db22e4f",
   "metadata": {},
   "source": [
    "The fit_transform method ensures that the scaler learns the mean and standard deviation from the data and scales it accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13d3e69-449f-43c8-a52b-ff1aba5e1897",
   "metadata": {},
   "source": [
    "### Splitting train\\test data:\n",
    "\n",
    "In the provided code snippet, the dataset is split into training and testing sets using the train_test_split function from scikit-learn. This is a common practice in machine learning to assess the performance of a model on unseen data. Let's discuss the key aspects of this section:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "a302a0b4-f206-4e40-9b51-388d9b710152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, data.target, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a5a7e1-db41-4c02-b1af-bfebe91bd864",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "The code uses a loop to iterate over each data point in the testing set (X_test). For each test data point, the predict_classification function is called. This function is presumably your own implementation of the KNN algorithm. The parameters passed include the training set (X_train and y_train), the test data point (X_test[i]), the number of neighbors (11 in this case), and the training labels (y_train).\r\n",
    "\r\n",
    "y_pred: It seems that y_pred is a list where the predicted labels for each test data point are being appended. This list likely contains the predicted labels for the entire testing set after the loop completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "c0672525-6dd6-4ee5-89b1-6ab74d5cc94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store predictions\n",
    "y_pred = list()\n",
    "\n",
    "# Making Predictions\n",
    "for i in range(len(X_test)):\n",
    "    y_pred.append(predict_classification(X_train, X_test[i], 11, y_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6a6bb6-9dac-4108-9d13-cb2141cc2c25",
   "metadata": {},
   "source": [
    "### Decision Tree Classification\n",
    "\n",
    "Implement and train the Decision Trees Classification model using scikit-learn. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "cf9ac823-136b-4769-b21c-fe34b17820b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-22 {color: black;}#sk-container-id-22 pre{padding: 0;}#sk-container-id-22 div.sk-toggleable {background-color: white;}#sk-container-id-22 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-22 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-22 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-22 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-22 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-22 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-22 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-22 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-22 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-22 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-22 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-22 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-22 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-22 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-22 div.sk-item {position: relative;z-index: 1;}#sk-container-id-22 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-22 div.sk-item::before, #sk-container-id-22 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-22 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-22 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-22 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-22 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-22 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-22 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-22 div.sk-label-container {text-align: center;}#sk-container-id-22 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-22 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-22\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" checked><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=10, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=10, random_state=42)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc_model = DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "dtc_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "130f131a-34c3-4271-85ec-1a2d9cbeb4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dtc_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "e3074040-e8fb-4d96-a819-0704fc374ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      " [[44  3]\n",
      " [ 7 60]]\n",
      "\n",
      "Accuray Score\n",
      " 0.9122807017543859\n",
      "\n",
      "F1 Score\n",
      " 0.923076923076923\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "asc = accuracy_score(y_test,y_pred) \n",
    "fs = f1_score(y_test,y_pred)\n",
    "\n",
    "print(\"Confusion Matrix\\n\",cm)\n",
    "print(\"\\nAccuray Score\\n\",asc)\n",
    "print(\"\\nF1 Score\\n\",fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c890b5-f0c8-4ca4-915c-886c02b1bd3a",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "063ff79b-8ebd-48e8-8e57-0b79fae6a525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9173420276354604"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(dtc_model, X_scaled, data.target, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92a7bba-40fd-4649-a16e-03684d100d7c",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "Results are presented in clear and concise tables, highlighting the performance metrics of each algorithm. We cross-reference these results with the experiments outlined in the methodology section to provide a comprehensive overview of the model evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd1a7a7-f84d-4e9a-a85b-5894060746a6",
   "metadata": {},
   "source": [
    "### KNN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "9512cdd5-b07c-469e-a7ac-41435a827cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      " [[43  4]\n",
      " [ 1 66]]\n",
      "\n",
      "Accuray Score\n",
      " 0.956140350877193\n",
      "\n",
      "F1 Score\n",
      " 0.9635036496350364\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "# Evaluate Model\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "asc = accuracy_score(y_test,y_pred) \n",
    "fs = f1_score(y_test,y_pred)\n",
    "print(\"Confusion Matrix\\n\",cm)\n",
    "print(\"\\nAccuray Score\\n\",asc)\n",
    "print(\"\\nF1 Score\\n\",fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6179fa8f-ed86-48bc-a8e0-5a6159c16da1",
   "metadata": {},
   "source": [
    "### KNN comparison with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "a01ccd38-3239-4c96-ac6e-41fe4e5e27ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the KNN classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=11)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = knn_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "063be7a8-556c-4ce2-a158-44cf1390e470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      " [[43  4]\n",
      " [ 1 66]]\n",
      "\n",
      "Accuray Score\n",
      " 0.956140350877193\n",
      "\n",
      "F1 Score\n",
      " 0.9635036496350364\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model\n",
    "cm = confusion_matrix(y_test,predictions)\n",
    "asc = accuracy_score(y_test,predictions) \n",
    "fs = f1_score(y_test,predictions)\n",
    "\n",
    "print(\"Confusion Matrix\\n\",cm)\n",
    "print(\"\\nAccuray Score\\n\",asc)\n",
    "print(\"\\nF1 Score\\n\",fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa5ffbe-1bc3-4fec-aa65-dc163d3e98ec",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "dbce2d51-c486-4cf4-be6b-f9ae0b483bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      " [[44  3]\n",
      " [ 2 65]]\n",
      "\n",
      "Accuray Score\n",
      " 0.956140350877193\n",
      "\n",
      "F1 Score\n",
      " 0.962962962962963\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "asc = accuracy_score(y_test,y_pred) \n",
    "fs = f1_score(y_test,y_pred)\n",
    "\n",
    "print(\"Confusion Matrix\\n\",cm)\n",
    "print(\"\\nAccuray Score\\n\",asc)\n",
    "print(\"\\nF1 Score\\n\",fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0d0933-6845-4e28-b1a2-43666feae40c",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "This section critically evaluates the strengths and weaknesses of each model. It demonstrates a nuanced understanding of the breast cancer dataset and the performance of the implemented algorithms. Emphasis is placed on how well the models achieve the stated aim of identifying the best machine learning model for breast cancer classification.\n",
    "\n",
    "**Evaluation and Parameter Tuning:** \r\n",
    "The KNN algorithm is applied to the breast cancer dataset, and the implementation's performance is assessed using evaluation metrics such as precision, accuracy, and recal\n",
    "\n",
    "**Comparison with scikit-learn's KNN:** The own implementation is compared with scikit-learn's KNN implementation to validate correctness and assess performance differences.l."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a498f46-8de3-42e7-b434-ad04b98cd7c4",
   "metadata": {},
   "source": [
    "## 6. Conclusions\n",
    "The findings of this study are succinctly summarized, relating them to the initial aim. The report provides insights into the effectiveness of each machine learning model on the breast cancer dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd47de0-cd8c-4617-a471-a77e323f0a4b",
   "metadata": {},
   "source": [
    "## 7. References\n",
    "Include any academic works or documentation referenced in the report."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
