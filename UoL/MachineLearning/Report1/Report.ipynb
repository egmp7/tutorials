{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7d91069-3689-433f-b2fe-f1f78fb02c9e",
   "metadata": {},
   "source": [
    "# CM3015 Mid-term Coursework Report\n",
    "\n",
    "## 1. Introduction\n",
    "Machine learning plays a crucial role in healthcare, and the identification of the best model for breast cancer classification is of paramount importance. This report aims to identify the best machine learning model for the breast cancer dataset obtained from the scikit-learn library. The study employs a comprehensive approach by implementing a k-Nearest Neighbors (KNN) algorithm from scratch, utilizing scikit-learn's KNN, and applying Decision Trees Classification from the same library. The report summarizes the findings and evaluations of these machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e428ca19-3db3-43ba-815c-75349cb5f698",
   "metadata": {},
   "source": [
    "## 2. Background\n",
    "In this study, two prominent machine learning algorithms, k-Nearest Neighbors (KNN) and Decision Trees Classification (DTC), are explored for their efficacy in breast cancer classification. KNN, a proximity-based algorithm, assigns a data point the majority class of its k-nearest neighbors[2]. On the other hand, DTC constructs a tree-like model, recursively partitioning the feature space to make decisions[3]. Both algorithms offer unique strengths and interpretability, and their performance will be rigorously compared to determine the most effective approach for breast cancer classification. The scratch implementation of KNN introduces an additional layer of analysis, providing insights into algorithmic intricacies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9122037-2d55-4813-a240-290adda2b39f",
   "metadata": {},
   "source": [
    "## 3. Methodology\n",
    "I began the investigation by importing the breast cancer dataset from scikit-learn, meticulously preparing the data through scaling, and strategically dividing it into training and testing sets. The detailed implementation of these processes is documented. The methodology includes the implementation of a KNN algorithm from scratch using standard Python code, alongside the application of scikit-learn's Decision Trees Classification. This section aims to present the implementations of the proposed processes, setting the stage for model comparison in the Results section. Our exploration will delve into each stage, transforming the data into the specific models aimed for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17386457-3757-4ddd-8b06-86d7940c14dd",
   "metadata": {},
   "source": [
    "### 3.1 Scaling \n",
    "\n",
    "To enhance the robustness and effectiveness of the models, a scaling process was employed. Utilizing the StandardScaler from scikit-learn, the feature values of the breast cancer dataset were transformed to conform to a standardized range, constraining them between 0 and 1. This normalization is crucial for preventing certain features from dominating the model due to disparate scales, ensuring that the distances between data points are not excessively influenced by specific columns. By constraining the values within a consistent range, we mitigate the risk of false positives or false negatives caused by skewed distances, thus contributing to the overall improvement of our model results.[6]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1ab76f1-5e82-4a62-98a9-e406c3e30d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "# Get features\n",
    "X = data.data\n",
    "# Instance of scaler\n",
    "scaler = StandardScaler()\n",
    "# Scaled features\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13d3e69-449f-43c8-a52b-ff1aba5e1897",
   "metadata": {},
   "source": [
    "### 3.2 Splitting train\\test data:\n",
    "\n",
    "The dataset undergoes division into training and testing sets through the utilization of the train_test_split function from scikit-learn. This widely adopted practice in machine learning serves the purpose of evaluating a model's efficacy when confronted with unseen data[1]. The method is configured to allocate 80% of the data for training and reserves the remaining 20% for testing. The specific dataset subjected to this partitioning is the normalized data X_scaled. This strategic splitting ensures a comprehensive evaluation of the model's generalization capabilities.[5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a302a0b4-f206-4e40-9b51-388d9b710152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, data.target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ada357-c816-45fb-9a8d-52db4f692e08",
   "metadata": {},
   "source": [
    "### 3.3 Homegrown KNN\n",
    "\n",
    "The implementation of the K-Nearest Neighbors (KNN) algorithm in Python involves crafting essential functions for distance computation and neighbor selection[2]. This homegrown KNN algorithm follows a systematic process comprising the following key steps:\n",
    "\n",
    "1. **Euclidean Distance Calculation:** The euclidean_dist() function computes the Euclidean distance between two data points. Specifically, it computes the distance between two rows of data, employing the fundamental concept of Euclidean geometry.\n",
    "\n",
    "3. **Neighbor Selection:** The get_neighbors_indexes()  function determines the k-nearest neighbors of a specified test data point within the training dataset by utilizing Euclidean distance. This function retrieves an array containing the respective indexes of the identified neighbors.\n",
    "\n",
    "   \n",
    "4. **Classification Prediction:** The predict_classification() function serves the critical role of predicting the class label of the test data point. Employing a majority voting mechanism among its k-nearest neighbors, this function ensures a robust classification prediction based on the collective input from the neighborhood.\n",
    "\n",
    "Note: The choice of the number of neighbors (k) is crucial, and various machine learning techniques, such as the gradient descent, aid in this calculation. However, the current library lacks the capability for the Homegrown model to connect and access gradient descent. Therefore, implementing gradient descent is outside the scope of this report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e35f4eb-f657-4481-b196-d6fb8c42dda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "356f572c-b4a1-4360-9c2f-619ed940a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distance between rows\n",
    "def euclidean_dist(row1, row2):\n",
    "    return np.sqrt(np.sum((row1 - row2)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "617e9fd9-c0b4-421d-9814-5e3c29574246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the neighbors and get their indexes\n",
    "def get_neighbors_indexes(sample, row, k):\n",
    "\n",
    "    # Instance for all distances of row\n",
    "    distances = list()\n",
    "\n",
    "    # For each row in sample set\n",
    "    for i, sample_row in enumerate(sample):\n",
    "        \n",
    "        # Calculate distance between rows\n",
    "        dist = euclidean_dist(row, sample_row)\n",
    "        # Store distance and index of sample_row in distances array \n",
    "        distances.append((i, dist))\n",
    "        # Sort them by distance\n",
    "        distances.sort(key=lambda tup: tup[1])\n",
    "\n",
    "    # Get the closest neighbors depending on k\n",
    "    neighbors_indexes = [index for index, _ in distances[:k]]\n",
    "    \n",
    "    return neighbors_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b467b9ce-f896-4187-b12e-ac769bd65f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a classification prediction with neighbors\n",
    "def predict_classification(sample, row, k, y_train):\n",
    "\n",
    "    # Instantiate votes\n",
    "    negative_counter = 0\n",
    "    positive_counter = 0\n",
    "\n",
    "    # Get neighbors\n",
    "    neighbor_indexes = get_neighbors_indexes(sample, row, k)\n",
    "\n",
    "    # Count votes\n",
    "    for n in neighbor_indexes:\n",
    "        if (y_train[n] == 0): negative_counter += 1\n",
    "        if (y_train[n] == 1): positive_counter += 1\n",
    "\n",
    "    # Classify\n",
    "    if (negative_counter > positive_counter): return 0\n",
    "    if (negative_counter < positive_counter): return 1\n",
    "    if (negative_counter == positive_counter): return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0aabde-a3e3-4653-baa6-9edb95dc041f",
   "metadata": {},
   "source": [
    "### 3.4 Predictions of the Homegrown KNN\n",
    "\n",
    "Now is the time to utilize the crafted KNN algorithm and gather the predictions, which will be stored in the variable homegrown_KNN_predictions. The methodology involves iterating through the test samples, comparing each test row with the training samples. The predict_classification() function is called each iteration and returns the classifications denoted by 0, 1, or None. This process offers valuable insights into the anticipated outcomes for each individual test data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0672525-6dd6-4ee5-89b1-6ab74d5cc94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store predictions\n",
    "homegrown_KNN_predictions = list()\n",
    "\n",
    "# Making Predictions\n",
    "for i in range(len(X_test)):\n",
    "    homegrown_KNN_predictions.append(predict_classification(X_train, X_test[i], 11, y_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6a6bb6-9dac-4108-9d13-cb2141cc2c25",
   "metadata": {},
   "source": [
    "### 3.5 Decision Tree Classification\n",
    "\r\n",
    "In this section, the implementation of a decision tree classification model is demonstrated, making use of the capabilities of the scikit-learn library for constructing the model.. The demonstration provides a clear illustration of how to instantiate and utilize the library for model development. The resultant model is stored in the variable dtc_model. By fitting the training data, previously segmented, this process ensures consistency in comparing with other algorithms. Furthermore, the predictions derived from this decision tree model are stored in the variable decision_tree_predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf9ac823-136b-4769-b21c-fe34b17820b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=10, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=10, random_state=42)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create a Decision Tree\n",
    "dtc_model = DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "# Fit data into model\n",
    "dtc_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "130f131a-34c3-4271-85ec-1a2d9cbeb4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "decision_tree_predictions = dtc_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92a7bba-40fd-4649-a16e-03684d100d7c",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "In this analysis, we unveil the performance of the Homegrown K-Nearest Neighbors (KNN) algorithm, meticulously crafted within this report. The algorithm's precision is highlighted through a comprehensive examination of confusion matrices, accuracy scores, and F1 scores. Additionally, a comparative analysis with scikit-learn's KNN model accentuates the homegrown solution's reliability. Furthermore, insights into the classification capabilities of a Decision Tree Classifier and cross-validation results contribute to a nuanced understanding of the models' effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03ad2c3d-9e67-40fa-823f-5eca15280799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69731098-8b1e-4f62-9ca1-c54fac97571f",
   "metadata": {},
   "source": [
    "### 4.1 Homegrown KNN Algorithm\n",
    "\n",
    "The outcomes of the Homegrown K-Nearest Neighbors (KNN) algorithm, developed within the scope of this report, are detailed below:\n",
    "\n",
    "**Confusion Matrix**\n",
    "```\n",
    "               | Predicted Negative | Predicted Positive |\n",
    "---------------|--------------------|--------------------|\n",
    "Actual Negative|        40          |         3          |\n",
    "---------------|--------------------|--------------------|\n",
    "Actual Positive|         2          |        69          |\n",
    "```\n",
    "\n",
    "**Accuracy Score**\n",
    "Approximately 95.61%\n",
    "\n",
    "**F1 Score**\n",
    "Approximately 96.50%\n",
    "\n",
    "These results attest to the robust performance of the Homegrown KNN algorithm. The high accuracy and balanced F1 score demonstrate the model's reliability in making accurate predictions on the dataset. The confusion matrix offers a comprehensive breakdown of accurate and inaccurate categorizations, further reinforcing the efficacy of the model in handling classification tasks. These numbers affirm the reliability of the model constructed in this report, validating its competence in making precise and consistent predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9512cdd5-b07c-469e-a7ac-41435a827cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      " [[40  3]\n",
      " [ 2 69]]\n",
      "\n",
      "Accuray Score\n",
      " 0.956140350877193\n",
      "\n",
      "F1 Score\n",
      " 0.965034965034965\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Homegrown KNN Model\n",
    "cm = confusion_matrix(y_test,homegrown_KNN_predictions)\n",
    "asc = accuracy_score(y_test,homegrown_KNN_predictions) \n",
    "fs = f1_score(y_test,homegrown_KNN_predictions)\n",
    "\n",
    "print(\"Confusion Matrix\\n\",cm)\n",
    "print(\"\\nAccuray Score\\n\",asc)\n",
    "print(\"\\nF1 Score\\n\",fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7f2218-d7c6-43db-86c9-2b8d2b79926e",
   "metadata": {},
   "source": [
    "### 4.2 Comparative Analysis: Homegrown KNN vs. SKlearn KNN\n",
    "\n",
    "In the pursuit of assessing the performance of the Homegrown K-Nearest Neighbors (KNN) algorithm, a comparative analysis was undertaken by creating a model utilizing the popular scikit-learn library, implementing its own KNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a01ccd38-3239-4c96-ac6e-41fe4e5e27ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create and train the KNN classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=11)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "sklearn_KNN_predictions = knn_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e329a7e5-23c7-4dba-a5f9-5e08d858560a",
   "metadata": {},
   "source": [
    "The evaluation of both the Homegrown KNN algorithm and the SKlearn KNN model yielded identical results:\n",
    "\n",
    "**Confusion Matrix:**\n",
    "```\n",
    "               | Predicted Negative | Predicted Positive |\n",
    "---------------|--------------------|--------------------|\n",
    "Actual Negative|        40          |         3          |\n",
    "---------------|--------------------|--------------------|\n",
    "Actual Positive|         2          |        69          |\n",
    "```\n",
    "\n",
    "**Accuracy Score:** Approximately 95.61%\n",
    "\n",
    "**F1 Score:** Approximately 96.50%\n",
    "\n",
    "The striking similarity in results between the Homegrown KNN and SKlearn KNN models suggests a high degree of consistency and correctness in their classification predictions. Both models achieved impressive accuracy and a balanced F1 score, indicating their effectiveness in handling the dataset.\n",
    "\n",
    "This congruence in performance underscores the reliability of the Homegrown KNN algorithm, showcasing its capacity to match the results produced by a widely-used library implementation. This comparison provides confidence in the accuracy and functionality of the Homegrown KNN algorithm, validating its efficacy in classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "063be7a8-556c-4ce2-a158-44cf1390e470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      " [[40  3]\n",
      " [ 2 69]]\n",
      "\n",
      "Accuray Score\n",
      " 0.956140350877193\n",
      "\n",
      "F1 Score\n",
      " 0.965034965034965\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model\n",
    "cm = confusion_matrix(y_test,sklearn_KNN_predictions)\n",
    "asc = accuracy_score(y_test,sklearn_KNN_predictions) \n",
    "fs = f1_score(y_test,sklearn_KNN_predictions)\n",
    "\n",
    "print(\"Confusion Matrix\\n\",cm)\n",
    "print(\"\\nAccuray Score\\n\",asc)\n",
    "print(\"\\nF1 Score\\n\",fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bca0e9-cdc9-41a7-8b2d-4358ded281cf",
   "metadata": {},
   "source": [
    "### 4.3 Decision Tree Classifier\n",
    "\n",
    "The outcomes of the Decision Tree Classifier are presented below, providing insights into its classification performance.\n",
    "\n",
    "**Confusion Matrix:**\n",
    "```\n",
    "               | Predicted Negative | Predicted Positive |\n",
    "---------------|--------------------|--------------------|\n",
    "Actual Negative|        40          |         3          |\n",
    "---------------|--------------------|--------------------|\n",
    "Actual Positive|         3          |        68          |\n",
    "```\n",
    "\n",
    "**Accuracy Score:** Approximately 94.74%\n",
    "\n",
    "**F1 Score:** Approximately 95.77%\n",
    "\n",
    "The Decision Tree Classifier exhibits robust classification performance, characterized by a strong accuracy score and a balanced F1 score. The model's accuracy reflects its effectiveness in making accurate predictions, while the balanced F1 score underscores its ability to maintain precision and recall equilibrium.\n",
    "\n",
    "These results affirm the Decision Tree Classifier's competence in handling classification tasks and contribute to a comprehensive understanding of its performance on the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbce2d51-c486-4cf4-be6b-f9ae0b483bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      " [[40  3]\n",
      " [ 3 68]]\n",
      "\n",
      "Accuray Score\n",
      " 0.9473684210526315\n",
      "\n",
      "F1 Score\n",
      " 0.9577464788732394\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model\n",
    "cm = confusion_matrix(y_test,decision_tree_predictions)\n",
    "asc = accuracy_score(y_test,decision_tree_predictions) \n",
    "fs = f1_score(y_test,decision_tree_predictions)\n",
    "\n",
    "print(\"Confusion Matrix\\n\",cm)\n",
    "print(\"\\nAccuray Score\\n\",asc)\n",
    "print(\"\\nF1 Score\\n\",fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252be956-f755-4ecd-9534-da6438f52045",
   "metadata": {},
   "source": [
    "### 4.4 Cross-Validation Results\n",
    "To comprehensively evaluate the generalization capabilities of the machine learning models, cross-validation was employed using the scikit-learn library. The decision tree classifier (dtc_model) and the K-Nearest Neighbors algorithm (knn_classifier) were subjected to this rigorous assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00147766-9545-4d05-95f1-998a18c33d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690572c0-04c2-4397-9160-55fafaf89f4e",
   "metadata": {},
   "source": [
    "**Decision Tree Classifier:**\n",
    "The decision tree classifier yielded an average cross-validation score of approximately 91.73%. This score reflects the model's performance across different folds, providing insights into its capacity to generalize data that has not been encountered before[4].The robust performance of the decision tree classifier indicates its reliability and effectiveness in making accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "063ff79b-8ebd-48e8-8e57-0b79fae6a525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9173420276354604"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc_model_scores = cross_val_score(dtc_model, X_scaled, data.target, cv=5)\n",
    "dtc_model_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fbb4a3-81dd-4702-909d-689d406978cc",
   "metadata": {},
   "source": [
    "**K-Nearest Neighbors Algorithm:**\r\n",
    "Unfortunately, the Homegrown K-Nearest Neighbors (KNN) algorithm was found to be incompatible with the scikit-learn library's cross-validation function. Nevertheless, to maintain a comparative analysis, the scikit-learn KNN classifier (knn_classifier) underwent cross-validation.\r\n",
    "\r\n",
    "The scikit-learn KNN model exhibited an impressive average cross-validation score of approximately 96.31%. This underscores the model's strong generalization capabilities, indicating its proficiency in handling diverse subsets of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b492622a-d8be-479b-9f67-b596c65c1195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9631113181183046"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc_model_scores = cross_val_score(knn_classifier, X_scaled, data.target, cv=5)\n",
    "dtc_model_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ccf647-6120-4b60-8ea5-1c949189b1f4",
   "metadata": {},
   "source": [
    "While the Homegrown KNN algorithm couldn't be directly integrated into the scikit-learn cross-validation process, the results from the scikit-learn KNN model suggest a high level of generalization, contributing to a comprehensive assessment of the models' performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a59a0a-e4c8-4aaf-a8d8-cd0186f51726",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "The evaluation of the machine learning models in this report reveals noteworthy insights into their performance. The Homegrown K-Nearest Neighbors (KNN) algorithm exhibits robustness, reflected in a high accuracy of approximately 95.61% and a balanced F1 score of 96.50%. The congruence with scikit-learn's KNN model suggests reliability, affirming the homegrown solution's competency.\n",
    "\n",
    "The Decision Tree Classifier, with an accuracy of about 94.74% and an F1 score of 95.77%, demonstrates commendable classification capabilities. These results emphasize the model's precision and recall equilibrium.\n",
    "\n",
    "Cross-validation further underscores the models' generalization capabilities. The decision tree classifier achieves an average score of around 91.73%, while scikit-learn's KNN model attains an impressive 96.31%. Although the Homegrown KNN algorithm doesn't integrate seamlessly into cross-validation, scikit-learn's KNN results suggest its potential for robust generalization.\n",
    "\n",
    "Collectively, these evaluations validate the effectiveness of the machine learning models in handling diverse datasets and showcase their potential for real-world applications.\n",
    "\n",
    "Selecting a winning algorithm entails a thoughtful consideration of various factors beyond raw performance metrics. While both the Homegrown K-Nearest Neighbors (KNN) algorithm and the Decision Tree Classifier showcase strong results on the breast cancer dataset in terms of accuracy and F1 score, other critical aspects need examination. Considerations include the interpretability of the model, computational efficiency, and ease of implementation. Moreover, assessing factors like model robustness, scalability, and potential for integration into cross-validation procedures can provide a more comprehensive understanding of each algorithm's suitability. Hence, a holistic approach considering a spectrum of criteria is essential to determine the most fitting machine learning model for the breast cancer dataset, ensuring alignment with the goals and constraints of the intended application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd47de0-cd8c-4617-a471-a77e323f0a4b",
   "metadata": {},
   "source": [
    "## 6. References\n",
    "[1] Andreas, Müller, and Sarah Guido. Introduction to Machine Learning with Python: A Guide for Data Scientists, 2016. \n",
    "\n",
    "[2] Scikit-Learn. “1.10. Decision Trees.” scikit. Accessed December 21, 2023. https://scikit-learn.org/stable/modules/tree.html. \n",
    "\n",
    "[3] Scikit-Learn. “1.6. Nearest Neighbors.” scikit. Accessed December 21, 2023. https://scikit-learn.org/stable/modules/neighbors.html. \n",
    "\n",
    "[4] Scikit-Learn. “Sklearn.Model_selection.Cross_val_score.” scikit. Accessed December 21, 2023. https://scikitlearn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score. \n",
    "\n",
    "[5] Scikit-Learn. “Sklearn.Model_selection.Train_test_split.” scikit. Accessed December 21, 2023. https://scikitlearn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split. \n",
    "\n",
    "[6] Scikit-Learn. “Sklearn.Preprocessing.StandardScaler.” scikit. Accessed December 21, 2023. https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html. 016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58efc485-26fa-4554-8c1e-a9693fca78f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
