{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Deep learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The IMDB dataset, a publicly available collection of movie reviews, has been extensively utilized in machine learning research endeavors. In line with this tradition, this report will delve into the methodologies and analyses required to construct a deep learning algorithm. Drawing inspiration from the workflow outlined in the book \"Deep Learning with Python,\" this report will navigate through the intricacies of algorithm creation, optimization techniques, and exploration of various hyperparameters. Subsequently, a thorough examination of the obtained results will be conducted, shedding light on the strengths and weaknesses inherent in this workflow.\n",
    "\n",
    "## 1 Defining the problem and assembling a dataset\n",
    "\n",
    "This report aims to develop a deep learning program designed to classify the sentiment of movie reviews as either positive or negative, thus framing the problem as a binary classification task. The IMDB dataset is chosen as an ideal candidate for this study due to its comprehensive collection of 50,000 movie reviews. Each review within the dataset undergoes a preprocessing step where words are mapped to numerical values, facilitating subsequent analysis and labeling.\n",
    "\n",
    "You hypothesize that your outputs can be predicted given your inputs.\n",
    "You hypothesize that your available data is sufficiently informative to learn the relationship between inputs and outputs.\n",
    "\n",
    "TO DO: show IMDB raw data\n",
    "\n",
    "\n",
    "## 2 Choosing a measure of success\n",
    "\n",
    "## 3 Deciding on an evaluation protocol\n",
    "\n",
    "## 4 Preparing your data\n",
    "\n",
    "## 5 Developing a model that does better than a baseline\n",
    "\n",
    "## 6 Scaling up: developing a model that overfits\n",
    "\n",
    "## 7 Regularizing your model and tuning your hyperparameters\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# Load data\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
