{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees\n",
    "\n",
    "This is an exploratory exercise to allow you to learn more about decision trees and how they might be used in scikit-learn.\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "* Go through the notebook and complete the tasks. \n",
    "* Make sure you understand the examples given. If you need help, refer to the documentation links provided or go to the discussion forum. \n",
    "* When a question allows a free-form answer (e.g. what do you observe?), create a new markdown cell below and answer the question in the notebook. \n",
    "* Save your notebooks when you are done.\n",
    "\n",
    "Before you do the tasks below, go through the scikit-learn decision tree tutorial <a href=\"https://scikit-learn.org/stable/modules/tree.html\">here</a>, with the classifier described <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">here</a>. The tutorial contains instructions on how to use decision trees for both classification and regression in Python. \n",
    "\n",
    "**Task 1:**\n",
    "Using what you learnt in the scikit-learn decision tree tutorial, use decision trees for classification on the iris dataset, and for regression on the diabetes dataset (both included in ```sklearn.datasets```). Your code should print the accuracy and the confusion matrix for the classification problem, and mean squared error for the regression. Try comparing the results for different maximum tree depths. \n",
    "\n",
    "Note: You should split your data 80% training and 20% for testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth: 2\n",
      "Accuracy: 0.9666666666666667\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  8  1]\n",
      " [ 0  0 11]]\n",
      "\n",
      "Max Depth: 3\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "\n",
      "Max Depth: 4\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "\n",
      "Max Depth: 5\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a list of maximum tree depths to experiment with\n",
    "max_depths = [2, 3, 4, 5]\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    # Create a Decision Tree classifier\n",
    "    clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "    \n",
    "    # Train the classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Max Depth: {max_depth}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now evaluate a regression on the diabetes dataset \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth: 2\n",
      "Mean Squared Error: 3866.038156768628\n",
      "\n",
      "Max Depth: 3\n",
      "Mean Squared Error: 3656.186930948001\n",
      "\n",
      "Max Depth: 4\n",
      "Mean Squared Error: 3594.089844855363\n",
      "\n",
      "Max Depth: 5\n",
      "Mean Squared Error: 3545.4104698436895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a list of maximum tree depths to experiment with\n",
    "max_depths = [2, 3, 4, 5]\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    # Create a Decision Tree regressor\n",
    "    reg = DecisionTreeRegressor(max_depth=max_depth, random_state=42)\n",
    "    \n",
    "    # Train the regressor\n",
    "    reg.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = reg.predict(X_test)\n",
    "    \n",
    "    # Calculate mean squared error\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Max Depth: {max_depth}\")\n",
    "    print(f\"Mean Squared Error: {mse}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:**\n",
    "How would you avoid overfitting in decision trees? Read the decision tree classifier described <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">here</a> for help. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid overfitting in decision trees, you can implement the following strategies:\n",
    "\n",
    "1. **Limit the maximum depth of the tree:** One of the simplest ways to prevent overfitting is to limit the maximum depth of the tree. This prevents the tree from becoming too complex and fitting the training data too closely.\n",
    "\n",
    "2. **Set a minimum number of samples per leaf node:** By specifying a minimum number of samples required to be at a leaf node, you can prevent the tree from creating nodes with very few samples, which might be noisy.\n",
    "\n",
    "3. **Prune the tree:** After the tree has been built, you can prune it by removing nodes that do not contribute significantly to improving the model's performance.\n",
    "\n",
    "4. **Use feature selection or dimensionality reduction techniques:** Before training the tree, you can perform feature selection or apply dimensionality reduction techniques to reduce the number of features used in the training process.\n",
    "\n",
    "5. **Use ensemble methods:** Ensemble methods like Random Forests or Gradient Boosting combine multiple decision trees to improve generalization and reduce overfitting.\n",
    "\n",
    "6. **Cross-validation:** Use techniques like k-fold cross-validation to evaluate the model's performance on different subsets of the data. This helps you get a more robust estimate of the model's performance.\n",
    "\n",
    "7. **Collect more data:** Having more data can help the model generalize better, as it has more examples to learn from.\n",
    "\n",
    "8. **Regularization techniques:** Some variants of decision trees, like Regularized Greedy Forests, include regularization terms that help prevent overfitting.\n",
    "\n",
    "9. **Early stopping:** During the training process, monitor the performance on a validation set and stop training once the performance plateaus or starts to degrade.\n",
    "\n",
    "Remember that the choice of which method(s) to use will depend on the specific problem, the nature of the data, and the behavior of the decision tree during training. It's often a good practice to try multiple approaches and evaluate their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
